---
layout: single
title: '"[DL] CNN의 작동원리를 알아보자 (1) - 기본원리 및 구조 이해하기"'
categories:
  - deep_learning
tags:
  - deep-learning
  - DL
  - 딥러닝
toc: true
highlight: false
header:
  teaser: /assets/images/review_article/deep_learing_thumbnail.jpg
  overlay_image: /assets/images/review_article/deep_learing_thumbnail.jpg
  overlay_filter: 0.5
  caption: "Photo credit: [**Unsplash**](https://unsplash.com/ko/%EC%82%AC%EC%A7%84/XJXWbfSo2f0)"
---

## 🚦 Summary


---

## 📌 Intro.
- 

---

> 💡 이런걸 정리합니다.


---

### CNN의 작동원리 이해하기 (1) - 필터 사이즈
- [이전 포스팅(Convolution을 이해하고 이미지 처리하기: 딥러닝의 기본 개념과 응용)](https://kimgabe.github.io/personal_study/deep_learning/Convolution_%EC%95%8C%EC%95%84%EB%B3%B4%EA%B8%B0/) 에서 영상분석의 방법을 정리하였습니다.
- 그리고 <font color="#00b050">영상의 특징을 잡아내는 방법은 필터 사이즈를 점점 키워나가는 방법과, 영상의 크기를 점점 줄여나가는 방법 2가지</font>가 있다고 정리하였고, 이 <span style="background:#fff88f">두가지가 복합적으로 적용되는것이 CNN</span>이라고 정리한 바 있습니다.
- 이중 `필터 사이즈를 점점 키워나가는 방법 (Convolution Layer를 여러개 쌓기)` 가 무엇인지에 대한 설명입니다.

![](https://i.imgur.com/5kWqlOr.png)

#### 🤓 [🤔 컨볼루션 연산과정](🤔%20컨볼루션%20연산과정.md) 의 특징
- 결합법칙이 성립합니다.
	- $(f\times g) \times h =f \times (g \times h)$
	- $(f\times g)$ 로 컨볼루션 연산을 하고 $h$와 컨볼루션 연산을 하나 $(g \times h)$ 로 컨볼루션 연산을 하고 나중에 $f$와 컨볼루션 연산을 하나 결과는 같다는 것입니다.
- 이 수식을 CNN의 컨볼루션 연산으로 치환해서 보자면
	- $f$ : (입력)영상 
	- $g$ : <font color="#ff0000">R</font>(ed)/<font color="#00b050">G</font>(reen) 통과시키는 필터
	- $h$ : <font color="#ff0000">R</font>(ed) 통과 필터

---

- 이 특징을 다시 적용하면서 위의 이미지를 풀어 보자면 아래의 두가지는 같은 값을 도출 합니다.
	- $f$라는 영상에 $g$를 컨볼루션 해서 $h$를 컨볼루션하는 것 : $(f\times g) \times h$
	- $g$와 $h$라는 필터를 먼저 컨볼루션한 뒤에 $f$라는 영상과 컨볼루션 하는 것 : $f \times (g \times h)$
- $3 \times 3$  과 $3 \times 3$을 컨볼루션 하면 $5 \times 5$ 형태가 됩니다.
- 이 개념을 적용해서 <font color="#00b050">Convolution Layer를 여러겹으로 쌓음으로써 필터의 크기를 점점 키울 수 있습니다.</font>
---
#### 어떻게 필터가 커지는 것인가?!
- 위 이미지의 1번 표시에는 입력영상($f$) 에 1번째 Convolution Layer인 $g$ 를 적용시킨 것으로 결과는 $3 \times 3$ 일 것입니다.
	- $f$에 $3 \times 3$ 크기의 필터 $g$를 적용한 결과로 $f \times g$ 에 해당합니다.
- 두번째로 $3 \times 3$ 에 $3 \times 3$인 $h$라는 Convolution Layer를 적용시켜 Convolution 연산을 하면 $5 \times 5$ 사이즈의 연산 결과가 나옵니다.
	- $f\times g$의 결과에 $3 \times 3$ 크기의 필터 $h$ 를 적용한 결과는 $f \times (g \times h)$가 됩니다.
	- 결합법칙에 의해 이 두개의 순서에 상관없이 같은 값을 가집니다.
- 세번째로 이 $5 \times 5$ 사이즈의 연산결과에 $3 \times 3$ 사이즈인 세번째 Convolution Layer를 적용시켜 convolution 연산을 하면 그 결과는 $7 \times 7$ 형태가 됩니다.
	- 세번째 필터를 계산하기 전에 $g \times h$ 를 계산하면, $f$에 $5 \times 5$ 크기의 결과를  하는 것과 같습니다.
- 이러한 과정을 거치면서 입력영상($f$) 는 3가지 크기의 필터에 적용을 받아 결과를 도출한 결과가 됩니다.
- 이것이 가능한 이유는 <span style="background:#affad1">Convolution 연산이 결합법칙이 성립되는 연산이기 때문</span>입니다.

---

### CNN의 작동원리 이해하기 (2) - 영상 사이즈를 점점 작게 만들기

![](https://i.imgur.com/nEdCeEi.png)


- 이를 위해 각 Convolution Layer 사이사이에 영상의 사이즈를 점점 작게 만드는 Max Pooling이라는 [풀링 레이어(Pooling Layer)](풀링%20레이어(Pooling%20Layer).md) 를 삽입해 이미지 크기를 줄입니다.
	- [풀링 레이어(Pooling Layer)](풀링%20레이어(Pooling%20Layer).md) 는 컨볼루션 계층에서 추출된 특징 맵의 크기를 줄이는 역할을 하는 레이어입니다.
	- Max Pooling은 이러한 [풀링 레이어(Pooling Layer)](풀링%20레이어(Pooling%20Layer).md) 의 한 종류로, 맵의 크기를 줄이는 방식입니다.
		- 일종의 하이퍼파라미터 같은 것이라 볼 수 있습니다.
		- 다른 풀링 레이어의 종류로는 Average Pooling 이라는 레이어도 있습니다.
	- Mas Pooling은 특정 영역에서 **가장 큰 값을 선택**하여 특징 맵의 크기를 줄입니다. 예를 들어, 2x2 영역에서 가장 큰 값을 선택합니다.



---

### CNN의 구조
![](https://i.imgur.com/dDNQyIS.png)

- 위 이미지는 초기의 CNN(합성곱 신경망) 구조 중 하나인 LeNet-5를 보여주고 있습니다. 이 네트워크는 손으로 쓴 숫자를 인식하기 위해 설계되었고, 아래와 같은 여러 층(layer)으로 구성되어 있습니다:

1. **입력(Input)**: 32x32 크기의 이미지가 네트워크에 입력됩니다. 이 이미지는 숫자를 포함하고 있으며, 네트워크의 목적은 이 숫자를 인식하는 것입니다.
    
2. **첫 번째 합성곱 층(C1 - Convolution Layer)**: 입력 이미지에 여러 개의 5x5 크기의 필터를 적용해 28x28 크기의 특징 맵(feature map) 6개를 생성합니다.
    
3. **첫 번째 맥스풀링 층(S2 - Maxpooling Layer)**: 특징 맵의 크기를 줄이기 위해 2x2 풀링을 적용해 14x14 크기의 특징 맵 6개를 생성합니다.
    
4. **두 번째 합성곱 층(C3 - Convolution Layer)**: 두 번째 합성곱 층에서는 더 많은 필터를 사용하여 10x10 크기의 특징 맵 16개를 생성합니다.
    
5. **두 번째 맥스풀링 층(S4 - Maxpooling Layer)**: 다시 풀링을 적용하여 5x5 크기의 특징 맵 16개로 줄입니다.
    
6. **완전 연결 층(C5 - Fully Connected Layer)**: 이제 합성곱과 풀링으로 추출한 특징들을 사용하여 120개의 노드로 이루어진 완전 연결 층을 거칩니다.
    
7. **완전 연결 층(F6 - Fully Connected Layer)**: 이전 층의 출력을 받아 84개의 노드로 구성된 또 다른 완전 연결 층을 거칩니다.
    
8. **출력(Output)**: 마지막으로, 네트워크는 10개의 노드로 구성된 출력 층으로, 각 노드는 하나의 숫자 클래스(0에서 9까지)를 나타냅니다. 소프트맥스 함수나 가우시안 연결(Gaussian connections)을 사용하여 최종적으로 어떤 숫자인지를 결정합니다.
    

이 네트워크는 합성곱 층과 맥스풀링 층을 통해 이미지에서 중요한 특징을 추출하고, 완전 연결 층을 통해 이 특징들을 사용하여 분류 작업을 수행합니다. LeNet-5는 딥러닝에서의 합성곱 신경망의 초기 모델 중 하나로, 현대 CNN의 기초를 제공한 중요한 구조입니다.


---

### CNN의 이해를 바탕으로 Art Style  논문 분석해보기
![](https://i.imgur.com/lLkNU7b.jpg)

- Art Style은 사진영상을 사전에 학습된 Art 영상의 특징처럼 변환해주는 일종의 Style Transfer 와 관련된 논문입니다.
- 즉 CNN을 이용해 하나의 이미지(콘텐츠 이미지)의 내용은 유지하면서 다른 이미지(스타일 이미지)의 스타일을 적용하는 기술입니다.
- 이 기술은 콘텐츠 이미지와 스타일 이미지를 CNN을 통과시켜 각각의 특징을 추출하고 이를 결합하여 새로운 이미지를 생성합니다.

#### 과정에 대한 설명
1. **스타일 이미지 추출**: 스타일 이미지에서는 텍스처, 색상, 브러시 스트로크 등과 같은 예술적 스타일 정보를 추출합니다. 이는 일반적으로 신경망의 초기 층에서 이루어지며, 스타일을 대표하는 특징들이 인코딩됩니다.
    
2. **콘텐츠 이미지 추출**: 콘텐츠 이미지에서는 건물의 색상, 경계선 등과 같은 구조적 특징을 추출합니다. 이러한 정보는 신경망의 더 깊은 층에서 추출되며, 이미지의 기본 틀을 형성하는 중요한 정보들입니다.
    
3. **특징 합성**: 추출된 스타일 정보를 콘텐츠 이미지의 구조적 특징과 합성하여, 콘텐츠 이미지는 그대로 유지하되 스타일 이미지의 예술적 특성을 입히게 됩니다.
    
4. **결과 생성**: 이 과정을 통해 생성된 결과물은 콘텐츠 이미지의 기본 구조는 유지하되, 스타일 이미지의 예술적 스타일이 적용된 새로운 이미지가 됩니다.
    

이 과정은 대개 컨볼루션 신경망(Convolutional Neural Network, CNN)을 사용하여 구현되며, 다양한 계층에서 특징을 추출하고 이를 결합하여 새로운 이미지를 생성합니다. 이미지에서 보여주는 `A1`, `B1`, `C1`등의 부분은 신경망의 각 층을 나타내며, 여기서 `A1 = B1`과 `h(A1) = h(C1)`는 스타일 이미지의 스타일이 콘텐츠 이미지에 성공적으로 전이되었음을 나타냅니다.

#### 계산식을 중심으로한 설명
스타일 전이에서 사용되는 신경망은 대체로 다음의 세 부분으로 나눌 수 있습니다:

1. **콘볼루션 계층(Convolution layer)**: 이미지에서 콘텐츠와 스타일 특징을 추출하는 역할을 합니다. 여러 필터(filter)를 통과하면서 이미지의 로컬 패턴을 인식합니다. 이 계층은 이미지의 세부적인 텍스처나 형태 등을 파악합니다.
    
2. **풀링 계층(Pooling layer)**: 콘볼루션 계층에서 추출한 특징들을 요약하고 간소화하는 역할을 합니다. Max Pooling은 주어진 영역에서 가장 높은 값을 선택하여 이미지의 크기를 줄이면서도 중요한 특징을 유지합니다. 이 과정은 네트워크의 과적합을 방지하고, 처리해야 할 특징의 수를 감소시키는 데 도움을 줍니다.
    
3. **전체 신경망(Full network)**: 콘볼루션과 풀링 계층을 거쳐 추출된 특징들을 기반으로 최종 이미지를 재구성합니다. 이때 콘텐츠 이미지의 구조적 정보와 스타일 이미지의 스타일 정보가 결합됩니다.
    

연산식 $A1 = B1$과 $h(A1) = h(C1)$을 설명하면 다음과 같습니다:

- **$A1 = B1$**: 여기서 $A1$과 $B1$은 신경망의 특정 계층에서 추출된 스타일 특징을 나타냅니다. $A1$은 원본 스타일 이미지에서, $B1$은 변환된 콘텐츠 이미지에서 추출된 스타일 특징입니다. $A1 = B1$은 변환된 이미지가 원본 스타일 이미지의 스타일 특징을 정확하게 모방했음을 의미합니다.
    
- **$h(A1) = h(C1)$**: $h$는 신경망을 통해 얻은 특징을 처리하는 함수로, 여기서는 스타일 특징을 콘텐츠 이미지에 적용하는 함수를 의미할 수 있습니다. $h(A1)$ 은 원본 스타일 이미지의 특징을 적용받은 결과이며, $C1$ 은 원본 콘텐츠 이미지에서 추출된 특징입니다. 이 식은 원본 콘텐츠 이미지의 구조적 특징에 스타일 이미지의 스타일을 적용한 결과가 원본 스타일 이미지의 특징과 동일하다는 것을 나타냅니다.
    

이 과정을 통해, 콘텐츠 이미지는 원본의 구조를 유지하면서 스타일 이미지의 예술적 스타일을 갖게 되는 것입니다.

#### 함수 $h$ 에 대한  추가 설명
함수 $h$ 는 주로 신경망의 중간 층의 특징 맵을 분석하여 이미지의 스타일을 캡처합니다. 이것이 가능한 이유는 다음과 같습니다
1. **중간 층의 특성**: CNN의 중간 층은 이미지의 질감과 패턴 같은 스타일적 특성을 포착합니다. 이러한 층들은 이미지의 상세한 콘텐츠보다는 질감과 스타일에 더 반응하기 때문입니다.
    
2. **그람 행렬(Gram Matrix)**: 스타일 정보는 그람 행렬을 통해 수학적으로 표현됩니다. 그람 행렬은 특징 맵의 각 필터 응답 사이의 내적을 계산하여 만듭니다. 이는 필터 응답의 패턴과 구조를 나타내는 통계적인 표현으로, 이미지의 스타일을 나타내는데 사용됩니다.
    
3. **최적화 과정**: 스타일 전이에서 원본 이미지에 대해 신경망을 통과시켜 얻은 특징 맵과 목표 스타일 이미지의 그람 행렬을 비교함으로써, 새로운 이미지의 스타일이 목표 스타일과 일치하도록 최적화합니다.
    

함수 $h$를 통해 스타일 정보를 얻는 과정은 다음과 같습니다:

- **특징 추출**: CNN을 통해 입력 이미지로부터 특징 맵을 추출합니다.
- **스타일 표현**: 중간 층의 특징 맵으로부터 그람 행렬을 계산합니다.
- **스타일 최적화**: 원본 콘텐츠 이미지의 특징 맵과 스타일 이미지의 그람 행렬을 비교하며, 이를 최적화 과정을 통해 일치시키도록 합니다.

이 과정을 통해 생성된 새로운 이미지는 원본 이미지의 콘텐츠를 유지하면서 목표 스타일을 반영하게 됩니다. 이러한 기술은 예술적 스타일을 다른 이미지에 적용하고자 할 때 특히 유용합니다.

---

## Convolution 레이어 구조 살펴보기(1)
![](https://i.imgur.com/7TyNlgP.png)

- 위에 개념을 이해할때는 1개짜리 레이어의 연산과정을 정리했지만, 실제사용되는 Convolution Layer에는 여러개의 Convolution 필터가 있습니다.
- <span style="background:#ff4d4f">한개의 필터 = 한 종류의 feature만 추출이 가능하기 때문입니다.</span>
- 하지만 제대로 이미지/영상의 특징을 추출해서 활용하려면 여러 종류의 feature를 추출하는 것이 학습에 더 도움이 될 것입니다.

![](https://i.imgur.com/2jFfmQq.png)

- $5 \times 5$ 크기의 필터가 16개 있다고 하면 위에 처럼 $16 \times 5 \times 5$ 라고 표기하고 이것을 하나의 [[텐서(Tensor)]]라고 합니다.
- 이 필터를 통과시키고 나면 16종의 feature를 추출할 수 있는 필터입니다.

### 이미지의 채널도 레이어의 개수에 영향을 미친다!
- 일반적으로 이미지의 크기를 `1920 x 1080`이라고 하면 이는 이미지의 크기가 `1920 x 1080`크기의 행렬로 이뤄졌다는 의미입니다.
- 하지만 이 이미지 데이터에는 숨겨진 정보가 있습니다. 바로 이미지의 채널입니다.
	- 컴퓨터에 있는 이미지 파일은 R,G,B라는 3가지 색상 값을 갖고 있습니다.
	- 이 3가지 색상을 이미지의 채널이라고 합니다.
- 간혹 빨강, 초록, 파랑에 1가지를 더해 투명도를 더해 4개의 채널로 저장하는 경우도 있습니다.
	- 혹은 흑백이미지처럼 단일 채널로 저장될 수도 있습니다.
- 정리하자면, <font color="#00b050">일반적으로 이미지의 채널은 3개이고, 하나의 픽셀을 나타내기 위해 3개의 값이 필요합니다.</font>
	- 예를들어 1920 x 1080 크기의 3채널 이미지를 나타내기 위해서는 6220800개(1920 x 1080 x 3)의 값이 필요합니다.
- 실제 CNN의 코드를 구현하고 그 레이어의 개수를 출력해보면 아래와 같습니다.

```python
import tensorflow as tf

# 1장 x 가로 1920 픽셀 x 세로 1080 픽셀 x 3채널(빨, 초, 파)
pic = tf.zeros((1, 1920, 1080, 3))
print("입력 이미지 데이터:", pic.shape)
pic_flatten_out = tf.keras.layers.Flatten()(pic)
print("이미지 데이터 값 개수: ", pic_flatten_out.shape)
print("\n")

single_conv_layer = tf.keras.layers.Conv2D(filters=1, # 1개 필터
                                    kernel_size=(5, 5),    # 5 x 5 크기
                                    use_bias=False)    # bias에 대해서는 여기서는 설명하지 않습니다.
single_conv_out = single_conv_layer(pic)
print("단일 필터 Convolution 레이어:", single_conv_layer.weights[0].shape)
print("단일 필터 Convolution 레이어의 파라미터 수:", single_conv_layer.count_params())
print("단일 필터 Convolution 결과 이미지:", single_conv_out.shape)
single_flatten_out = tf.keras.layers.Flatten()(single_conv_out)
print("단일 필터 Convolution 결과 이미지 데이터 수: ", single_flatten_out.shape)
print("\n")

multiple_conv_layer = tf.keras.layers.Conv2D(filters=16, # 16개 필터
                                    kernel_size=(5, 5),    # 5 x 5 크기
                                    use_bias=False)    # bias에 대해서는 여기서는 설명하지 않습니다.
multiple_conv_out = multiple_conv_layer(pic)
print("16개 필터 Convolution 레이어:", multiple_conv_layer.weights[0].shape)
print("16개 필터 Convolution 레이어의 파라미터 수:", multiple_conv_layer.count_params())
print("16개 필터 Convolution 결과 이미지:", multiple_conv_out.shape)
multiple_flatten_out = tf.keras.layers.Flatten()(multiple_conv_out)
print("16개 필터 Convolution 결과 이미지 데이터 수:", multiple_flatten_out.shape)
```
```
입력 이미지 데이터: (1, 1920, 1080, 3)
이미지 데이터 값 개수:  (1, 6220800)


단일 필터 Convolution 레이어: (5, 5, 3, 1)
단일 필터 Convolution 레이어의 파라미터 수: 75
단일 필터 Convolution 결과 이미지: (1, 1916, 1076, 1)
단일 필터 Convolution 결과 이미지 데이터 수:  (1, 2061616)


16개 필터 Convolution 레이어: (5, 5, 3, 16)
16개 필터 Convolution 레이어의 파라미터 수: 1200
16개 필터 Convolution 결과 이미지: (1, 1916, 1076, 16)
16개 필터 Convolution 결과 이미지 데이터 수: (1, 32985856)
```

- 위 결과에서 주목해야 할 것은 <font color="#00b050">Convolution layer의 weight가 필터의 개수에 따라서도 달라지고, 입력 이미지의 채널에 따라서도 달라진다는 점</font>입니다. 특히 <font color="#00b050">입력 이미지의 채널에 따라 달라지기도</font> 합니다.
- 채널이 2개라고 가정하면 아래 이미지처럼 Convolution 필터도 2채널이 필요합니다. 
  ![](https://i.imgur.com/PLefPAp.png)

✔️ <font color="#c0504d">필터가 2개인 것</font>과 <font color="#00b0f0">필터의 채널이 2인 것</font>이 <font color="#00b050">다르니 혼동될 수 있으니 유의해야 합니다.</font>


## Convolution  레이어 구조 살펴보기 (2)
### Max Pooling에 stride를 적용하기
- [풀링 레이어(Pooling Layer)](풀링%20레이어(Pooling%20Layer).md) 의 한 종류로 [맥스 풀링 (Max Pooling)](맥스%20풀링%20(Max%20Pooling).md) 이라는 것이 있습니다.
- 일반적으로 [CNN(Convolution Neural Network)](CNN(Convolution%20Neural%20Network).md) 을 할때 [맥스 풀링 (Max Pooling)](맥스%20풀링%20(Max%20Pooling).md) 은 [스트라이드(Stride)](스트라이드(Stride).md) 와 함께 사용됩니다.
- ![스트라이드(Stride)](스트라이드(Stride).md)
- [🤔 컨볼루션 연산과정](🤔%20컨볼루션%20연산과정.md) 에서는 필터가 한 칸씩 이동하며 연산을 수행하는데, 이는 [스트라이드(Stride)](스트라이드(Stride).md) 값이 1인 경우 입니다.
- 즉 [스트라이드(Stride)](스트라이드(Stride).md) 값을 조정함에 따라 한번에 shift하는 칸의 수를 조절할 수 있습니다. 아래의 이미지는 [스트라이드(Stride)](스트라이드(Stride).md) 값을 2로 한 경우의 [🤔 컨볼루션 연산과정](🤔%20컨볼루션%20연산과정.md) 을 보여줍니다.

![](https://i.imgur.com/xGBFiNP.png)

### Max Pooling과 Stride는 보통 같은 값을 가진다.
- 즉, [맥스 풀링 (Max Pooling)](맥스%20풀링%20(Max%20Pooling).md) 의 값도 무조건 1로 하는 것이 아니라 [스트라이드(Stride)](스트라이드(Stride).md) 의 값 처럼 조정이 가능하다는 겁니다.
- 또한, 서로 다른 값을 각각 지정할 수도 있다는 것입니다.
- 아래 이미지는 [맥스 풀링 (Max Pooling)](맥스%20풀링%20(Max%20Pooling).md) 값은 고정인데 [스트라이드(Stride)](스트라이드(Stride).md) 값이 달라질 때 어떻게 결과가 달라지는 지를 보여 줍니다.

![](https://i.imgur.com/idYaZ8I.png)

- 일반적으로 [맥스 풀링 (Max Pooling)](맥스%20풀링%20(Max%20Pooling).md) 은 [스트라이드(Stride)](스트라이드(Stride).md) 와 같도록 설정합니다.
	- 그 이유는 겹쳐서 풀링하지 않기 위함입니다.
	- 풀링의 크기와 스트라이드의 크기에 따라 결과의 크기가 달라지기 때문입니다.

#### [맥스 풀링 (Max Pooling)](맥스%20풀링%20(Max%20Pooling).md) 와 [스트라이드(Stride)](스트라이드(Stride).md) 로 변경되는 사이즈
- $2 \times 2$ max pooling 을 2 strid로 적용하면 가로와 세로가 각각 절반이 됩니다.
	- 면적으로는 $\frac{1}{4}$  가 됩니다.
- 아래의 샘플 코드로 두 값의 조정에 따른 결과를 도출할 수 있습니다.

```python
import tensorflow as tf
# 1장 x 가로 1920 픽셀 x 세로 1080 픽셀 x 3채널(빨, 초, 파)
pic = tf.zeros((1, 1920, 1080, 3))
print("입력 이미지 데이터:", pic.shape)
print('\n')

conv_layer = tf.keras.layers.Conv2D(filters=16, # 16개 필터
                                    kernel_size=(5, 5),    # 5 x 5 크기
                                    use_bias=False)    # bias에 대해서는 여기서는 설명하지 않습니다.
conv_out = conv_layer(pic)
print("16개 필터 Convolution 결과 데이터:", conv_out.shape)
print('\n')

pool_layer = tf.keras.layers.MaxPool2D(pool_size=(2, 2), strides=(2, 2))
pool_out = pool_layer(conv_out)
print("Pooling 결과 데이터:", pool_out.shape)
```
```
입력 이미지 데이터: (1, 1920, 1080, 3)


16개 필터 Convolution 결과 데이터: (1, 1916, 1076, 16)


Pooling 결과 데이터: (1, 958, 538, 16)
```

- 이는 두 [[하이퍼파라미터(Hyperparameter)]] 가 작동하는 방식에 의해서 결정됩니다.
	- [맥스 풀링 (Max Pooling)](맥스%20풀링%20(Max%20Pooling).md) 은 입력 데이터에 대해 지정된 크기(ex. $2\times 2$) 영역에서 최대값을 선택해 대표값으로 출력하는 역할입니다.
		- 즉 [맥스 풀링 (Max Pooling)](맥스%20풀링%20(Max%20Pooling).md) 의 값이 커질수록 더 넓은 범위에서 최대값을 탐색하고 출력합니다.
	- [스트라이드(Stride)](스트라이드(Stride).md) 는 그 [맥스 풀링 (Max Pooling)](맥스%20풀링%20(Max%20Pooling).md) 이 입력 데이터안에서 한번에 몇칸을 움직일지를 설정합니다.
		- 스트라이드값이 2라면 필터는 가로 & 세로 각각 2칸씩 이동합니다.

## 🎈 Outro.
- 
